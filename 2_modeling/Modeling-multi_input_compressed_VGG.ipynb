{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# general Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import concatenate, Input\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "# import helper functions\n",
    "# add our pipeline folder to the path to import functions\n",
    "sys.path.insert(1, '../2_modeling')\n",
    "from model_helper_no_cache import *\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters and directories to save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Multi_input'\n",
    "number_of_images = 15000\n",
    "EPOCHS = 30\n",
    "neurons_per_dense = 1024\n",
    "dense_layers = 2\n",
    "open_layers = 2\n",
    "penalty_weight = 10\n",
    "csv = '../1_cleaning/metadata_cleaned3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode all the categorical variables, and get the image paths\n",
    "X_train_images, X_val_images, X_train_hier, X_val_hier, y_train_bin, y_val_bin, features = train_test_split_custom_2(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image pipeline and create dataset to feed model\n",
    "dataset_multi_train = create_dataset_multi(X_train_images,X_train_hier, y_train_bin)\n",
    "dataset_multi_val = create_dataset_multi(X_val_images,X_val_hier, y_val_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming conventions\n",
    "Files should be saved in the following convention, which is outlined in the readme file\n",
    "model#_sample_size#_epoch#_dense#_trainable_layers_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the name of the model\n",
    "name_of_model = 'model_' + model_id + \\\n",
    "                '_sample_size_' + str(number_of_images) + \\\n",
    "                '_epoch_' + str(EPOCHS) + \\\n",
    "                '_dense_' + str(dense_layers) + \\\n",
    "                '_neurons_' + str(neurons_per_dense) + \\\n",
    "                '_losswbc_' + \\\n",
    "                '_num_open_layers_' + str(open_layers) + \\\n",
    "                '_penalty_weight_' + str(penalty_weight)\n",
    "\n",
    "# Send everything to the efs\n",
    "base_path = '/home/ubuntu/efs/models/'\n",
    "    \n",
    "# Directories for checkpoint\n",
    "checkpoint_path = base_path + 'Checkpoints/' + name_of_model + '.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# For training history\n",
    "csv_logger = CSVLogger(base_path + 'Training_history/' + name_of_model + \"_history_log.csv\", append=True)\n",
    "training_history_path = base_path + 'Training_history/' + name_of_model + '.pickle'\n",
    "\n",
    "# For model saving once training has ended\n",
    "saved_model_path = base_path + 'Saved_models/' + name_of_model + '.h5'\n",
    "\n",
    "# Model Image path\n",
    "model_image_path = base_path + 'Model_image/' + model_id + '.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the VGG19 pretrained network as seperate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label = y_train_bin.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the single layer Dense as it's own model\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(224,224,3))\n",
    "inputB = Input(shape=X_train_hier.shape[1], name=\"hier\")\n",
    "# the first branch operates on the first input\n",
    "x = VGG19(include_top=False, weights='imagenet')(inputA)\n",
    "x = Flatten()(x)\n",
    "x = Dense(neurons_per_dense, activation='relu', name='fc_1')(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(32, activation='relu', name = 'fc_hier')(inputB)\n",
    "y = Dense(32, activation=\"relu\")(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(neurons_per_dense, activation=\"relu\")(combined)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(num_label, activation=\"sigmoid\")(z)\n",
    "z = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "optimizer = Adam(lr=LR)\n",
    "loss = weighted_bce\n",
    "metrics = [accuracy_on_one, accuracy_on_zero, precision_on_1]\n",
    "\n",
    "z.compile(optimizer=optimizer, \n",
    "          loss=loss, \n",
    "          metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "tf.keras.utils.plot_model(\n",
    "    z,\n",
    "    to_file=model_image_path,\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "img = mpimg.imread(model_image_path)\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the multi-input model from https://www.programcreek.com/python/example/89660/keras.layers.concatenate\n",
    "\n",
    "\n",
    "def getMultiModel():\n",
    "    \n",
    "    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "\n",
    "    transfer_layer = vgg.get_layer('block5_pool')\n",
    "\n",
    "    # cutting of the end of the model before the dense layers\n",
    "    conv_model = Model(inputs=vgg.input,\n",
    "                       outputs=transfer_layer.output)\n",
    "\n",
    "    # Setup Trainable layers\n",
    "    conv_model.trainable = False if open_layers < 1 else True\n",
    "\n",
    "    for layer in conv_model.layers[:-(open_layers+1):]:\n",
    "        layer.trainable =  False\n",
    "    \n",
    "    x = conv_model.output\n",
    "    \n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu', name='fc_1')(x)\n",
    "    \n",
    "    input_2 = Input(shape=X_train_hier.shape[1], name=\"hier\")\n",
    "    \n",
    "    hier_layer = Dense(32, activation='relu', name = 'fc_hier')(input_2)\n",
    "    \n",
    "    merge_one = concatenate([x, hier_layer])\n",
    "    \n",
    "    merge_one = Dense(512, activation='relu', name='fc_2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(num_label, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    # Define the models\n",
    "    model = Model(inputs=[conv_model.input, input_2], outputs=predictions)\n",
    "    \n",
    "    # Settings\n",
    "    LR = 1e-5\n",
    "    optimizer = Adam(lr=LR)\n",
    "    loss = weighted_bce\n",
    "    metrics = [accuracy_on_one, accuracy_on_zero, precision_on_1]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_input_model = getMultiModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6f5f7d6668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    multi_input_model,\n",
    "    to_file=model_image_path,\n",
    "    show_shapes=False,\n",
    "    show_layer_names=False,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "img = mpimg.imread(model_image_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_input_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False:\tinput_1\n",
      "False:\tblock1_conv1\n",
      "False:\tblock1_conv2\n",
      "False:\tblock1_pool\n",
      "False:\tblock2_conv1\n",
      "False:\tblock2_conv2\n",
      "False:\tblock2_pool\n",
      "False:\tblock3_conv1\n",
      "False:\tblock3_conv2\n",
      "False:\tblock3_conv3\n",
      "False:\tblock3_conv4\n",
      "False:\tblock3_pool\n",
      "False:\tblock4_conv1\n",
      "False:\tblock4_conv2\n",
      "False:\tblock4_conv3\n",
      "False:\tblock4_conv4\n",
      "False:\tblock4_pool\n",
      "False:\tblock5_conv1\n",
      "False:\tblock5_conv2\n",
      "True:\tblock5_conv3\n",
      "True:\tblock5_conv4\n",
      "True:\tblock5_pool\n",
      "True:\tflatten\n",
      "True:\thier\n",
      "True:\tfc_1\n",
      "True:\tfc_hier\n",
      "True:\tconcatenate\n",
      "True:\tfc_2\n",
      "True:\tdropout\n",
      "True:\tdense\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(multi_input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_input_model.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a callback checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multi_input_model.fit(dataset_multi_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=dataset_multi_val,\n",
    "                    callbacks=[cp_callback,\n",
    "                               csv_logger,\n",
    "                               #sanity_check_callback,\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load old weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights from the checkpoint path above\n",
    "# new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets (incl. image pre-processing, resizing, putting into batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save history and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "\n",
    "pickle.dump(history.history, open(training_history_path, 'wb'))\n",
    "\n",
    "# Save the model\n",
    "multi_input_model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/efs/models/Saved_models/model_Multi_input_sample_size_15000_epoch_10_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.h5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
