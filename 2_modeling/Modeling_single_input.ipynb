{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a single input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import CSVLogger, LambdaCallback\n",
    "import sys\n",
    "from model_helper_no_cache import *\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters and directories to save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'ALL_FEATURES'\n",
    "number_of_images = 'all'\n",
    "EPOCHS = 20\n",
    "neurons_per_dense = 1024\n",
    "dense_layers = 2\n",
    "open_layers = 2\n",
    "penalty_weight = 10\n",
    "csv = '../1_cleaning/metadata_cleaned3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # Choose the batch size before each weight update\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunk of 1024 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split, plus the features that are used to binarize the data\n",
    "X_train, X_val, y_train_bin, y_val_bin, features = train_test_split_custom(number_of_images, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6671, 709)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_bin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming conventions\n",
    "Files should be saved in the following convention, which is outlined in the readme file\n",
    "model#_sample_size#_epoch#_dense#_trainable_layers_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the name of the model\n",
    "name_of_model = 'model_' + model_id + \\\n",
    "                '_sample_size_' + str(number_of_images) + \\\n",
    "                '_epoch_' + str(EPOCHS) + \\\n",
    "                '_dense_' + str(dense_layers) + \\\n",
    "                '_neurons_' + str(neurons_per_dense) + \\\n",
    "                '_losswbc_' + \\\n",
    "                '_num_open_layers_' + str(open_layers) + \\\n",
    "                '_penalty_weight_' + str(penalty_weight)\n",
    "\n",
    "# Send everything to the efs\n",
    "base_path = '/home/ubuntu/efs/models/'\n",
    "    \n",
    "# Directories for checkpoint\n",
    "checkpoint_path = base_path + 'Checkpoints/' + name_of_model + '.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# For training history\n",
    "csv_logger = CSVLogger(base_path + 'Training_history/' + name_of_model + \"_history_log.csv\", append=True)\n",
    "training_history_path = base_path + 'Training_history/' + name_of_model + '.pickle'\n",
    "\n",
    "# For model saving once training has ended\n",
    "saved_model_path = base_path + 'Saved_models/' + name_of_model + '.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the VGG19 pretrained network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = vgg.get_layer('block5_pool')\n",
    "\n",
    "# cutting of the end of the model before the dense layers\n",
    "conv_model = Model(inputs=vgg.input,\n",
    "                   outputs=transfer_layer.output)\n",
    "\n",
    "# freeze VGG\n",
    "conv_model.trainable = False if open_layers < 1 else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the number of trainable layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_model.layers[:-(open_layers+1):]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in conv_model.layers:\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label = y_train_bin.shape[1]\n",
    "\n",
    "def create_model():\n",
    "    # Start a new Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the convolutional part of the VGG16 model from above.\n",
    "    model.add(conv_model)\n",
    "\n",
    "    # Flatten the output of the VGG16 model because it is from a\n",
    "    # convolutional layer.\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a dense (aka. fully-connected) layer.\n",
    "    # This is for combining features that the VGG16 model has\n",
    "    # recognized in the image.\n",
    "    model.add(Dense(neurons_per_dense, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(neurons_per_dense, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_label, activation='sigmoid'))\n",
    "    \n",
    "    # Settings\n",
    "    LR = 1e-5\n",
    "    optimizer = Adam(lr=LR)\n",
    "    loss = weighted_bce\n",
    "    metrics = [accuracy_on_one, accuracy_on_zero, precision_on_1]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:\tmodel_1\n",
      "True:\tflatten_1\n",
      "True:\tdense_3\n",
      "True:\tdropout_2\n",
      "True:\tdense_4\n",
      "True:\tdropout_3\n",
      "True:\tdense_5\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 709)               726725    \n",
      "=================================================================\n",
      "Total params: 47,491,845\n",
      "Trainable params: 32,187,077\n",
      "Non-trainable params: 15,304,768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a callback checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load old weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights from the checkpoint path above\n",
    "# new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets (incl. image pre-processing, resizing, putting into batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "# new_model.load_weights(checkpoint_path)\n",
    "train_ds = create_dataset(X_train, y_train_bin, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)\n",
    "val_ds = create_dataset(X_val, y_val_bin, BATCH_SIZE, SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6671, 709)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_bin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\n\\nfile = os.path.join(base_path, \\'Training_history/\\', \"sanity_check_callback_log.json\")\\njson_log = open(file, mode=\\'wt\\', buffering=1)\\n\\nsanity_check_callback = LambdaCallback(\\n    on_epoch_begin=lambda epoch, logs: json_log.write(\\n        json.dumps({\\'epoch\\': epoch, \\'loss\\': logs}) + \\'\\n\\'),\\n    on_epoch_end=lambda epoch, logs: json_log.write(\\n        json.dumps({\\'epoch\\': epoch, \\'loss\\': logs}) + \\'\\n\\'))\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "file = os.path.join(base_path, 'Training_history/', \"sanity_check_callback_log.json\")\n",
    "json_log = open(file, mode='wt', buffering=1)\n",
    "\n",
    "sanity_check_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'epoch': epoch, 'loss': logs}) + '\\n'),\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'epoch': epoch, 'loss': logs}) + '\\n'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "      1/Unknown - 2s 2s/step - loss: 0.8936 - accuracy_on_one: 0.4743 - accuracy_on_zero: 0.5012 - precision_on_1: 0.0204"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the new model and start training\n",
    "history = new_model.fit(train_ds,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=create_dataset(X_val, y_val_bin),\n",
    "                    callbacks=[cp_callback,\n",
    "                               csv_logger,\n",
    "                               #sanity_check_callback,\n",
    "                              ])\n",
    "\n",
    "#json_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save history and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "\n",
    "pickle.dump(history.history, open(training_history_path, 'wb'))\n",
    "\n",
    "# Save the model\n",
    "new_model.save(saved_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
