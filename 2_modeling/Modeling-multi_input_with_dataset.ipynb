{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# general Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import concatenate, Input\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "# import helper functions\n",
    "# add our pipeline folder to the path to import functions\n",
    "sys.path.insert(1, '../2_modeling')\n",
    "from model_helper_no_cache import *\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters and directories to save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Multi_input'\n",
    "number_of_images = 'all'\n",
    "EPOCHS = 30\n",
    "neurons_per_dense = 1024\n",
    "dense_layers = 2\n",
    "open_layers = 2\n",
    "penalty_weight = 10\n",
    "csv = '../1_cleaning/metadata_cleaned3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode all the categorical variables, and get the image paths\n",
    "X_train_images, X_val_images, X_train_hier, X_val_hier, y_train_bin, y_val_bin, features = train_test_split_custom_2(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image pipeline and create dataset to feed model\n",
    "dataset_multi_train = create_dataset_multi(X_train_images,X_train_hier, y_train_bin)\n",
    "dataset_multi_val = create_dataset_multi(X_val_images,X_val_hier, y_val_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming conventions\n",
    "Files should be saved in the following convention, which is outlined in the readme file\n",
    "model#_sample_size#_epoch#_dense#_trainable_layers_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the name of the model\n",
    "name_of_model = 'model_' + model_id + \\\n",
    "                '_sample_size_' + str(number_of_images) + \\\n",
    "                '_epoch_' + str(EPOCHS) + \\\n",
    "                '_dense_' + str(dense_layers) + \\\n",
    "                '_neurons_' + str(neurons_per_dense) + \\\n",
    "                '_losswbc_' + \\\n",
    "                '_num_open_layers_' + str(open_layers) + \\\n",
    "                '_penalty_weight_' + str(penalty_weight)\n",
    "\n",
    "# Send everything to the efs\n",
    "base_path = '/home/ubuntu/efs/models/'\n",
    "    \n",
    "# Directories for checkpoint\n",
    "checkpoint_path = base_path + 'Checkpoints/' + name_of_model + '.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# For training history\n",
    "csv_logger = CSVLogger(base_path + 'Training_history/' + name_of_model + \"_history_log.csv\", append=True)\n",
    "training_history_path = base_path + 'Training_history/' + name_of_model + '.pickle'\n",
    "\n",
    "# For model saving once training has ended\n",
    "saved_model_path = base_path + 'Saved_models/' + name_of_model + '.h5'\n",
    "\n",
    "# Model Image path\n",
    "model_image_path = base_path + 'Model_image/' + model_id + '.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the VGG19 pretrained network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the multi-input model from https://www.programcreek.com/python/example/89660/keras.layers.concatenate\n",
    "num_label = y_train_bin.shape[1]\n",
    "\n",
    "def getMultiModel():\n",
    "    \n",
    "    base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    \n",
    "    # Setup the trainability of the VGG\n",
    "    base_model.trainable = False if open_layers < 1 else True\n",
    "    \n",
    "    for layer in base_model.layers[:-(open_layers+1):]:\n",
    "        layer.trainable =  False\n",
    "    \n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(neurons_per_dense, activation='relu', name='fc_1')(x)\n",
    "    \n",
    "    input_2 = Input(shape=X_train_hier.shape[1], name=\"hier\")\n",
    "    \n",
    "    hier_layer = Dense(32, activation='relu', name = 'fc_hier')(input_2)\n",
    "    \n",
    "    merge_one = concatenate([x, hier_layer])\n",
    "    \n",
    "    merge_one = Dense(neurons_per_dense, activation='relu', name='fc_2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(num_label, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    # Define the models\n",
    "    model = Model(inputs=[base_model.input, input_2], outputs=predictions)\n",
    "    \n",
    "    # Settings\n",
    "    LR = 1e-5\n",
    "    optimizer = Adam(lr=LR)\n",
    "    loss = weighted_bce\n",
    "    metrics = [accuracy_on_one, accuracy_on_zero, precision_on_1]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_input_model = getMultiModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcd260f62e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    multi_input_model,\n",
    "    to_file=model_image_path,\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "img = mpimg.imread(model_image_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_input_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False:\tinput_1\n",
      "False:\tblock1_conv1\n",
      "False:\tblock1_conv2\n",
      "False:\tblock1_pool\n",
      "False:\tblock2_conv1\n",
      "False:\tblock2_conv2\n",
      "False:\tblock2_pool\n",
      "False:\tblock3_conv1\n",
      "False:\tblock3_conv2\n",
      "False:\tblock3_conv3\n",
      "False:\tblock3_conv4\n",
      "False:\tblock3_pool\n",
      "False:\tblock4_conv1\n",
      "False:\tblock4_conv2\n",
      "False:\tblock4_conv3\n",
      "False:\tblock4_conv4\n",
      "False:\tblock4_pool\n",
      "False:\tblock5_conv1\n",
      "False:\tblock5_conv2\n",
      "True:\tblock5_conv3\n",
      "True:\tblock5_conv4\n",
      "True:\tblock5_pool\n",
      "True:\tflatten\n",
      "True:\thier\n",
      "True:\tfc_1\n",
      "True:\tfc_hier\n",
      "True:\tconcatenate\n",
      "True:\tfc_2\n",
      "True:\tdropout\n",
      "True:\tdense\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(multi_input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_input_model.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a callback checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "    417/Unknown - 158s 380ms/step - loss: 0.7453 - accuracy_on_one: 0.8235 - accuracy_on_zero: 0.7915 - precision_on_1: 0.2627\n",
      "Epoch 00001: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 199s 477ms/step - loss: 0.7453 - accuracy_on_one: 0.8235 - accuracy_on_zero: 0.7915 - precision_on_1: 0.2627 - val_loss: 0.0000e+00 - val_accuracy_on_one: 0.0000e+00 - val_accuracy_on_zero: 0.0000e+00 - val_precision_on_1: 0.0000e+00\n",
      "Epoch 2/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.5556 - accuracy_on_one: 0.8775 - accuracy_on_zero: 0.8594 - precision_on_1: 0.3437\n",
      "Epoch 00002: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 193s 464ms/step - loss: 0.5556 - accuracy_on_one: 0.8775 - accuracy_on_zero: 0.8594 - precision_on_1: 0.3437 - val_loss: 0.4716 - val_accuracy_on_one: 0.8987 - val_accuracy_on_zero: 0.8790 - val_precision_on_1: 0.3834\n",
      "Epoch 3/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.5038 - accuracy_on_one: 0.8918 - accuracy_on_zero: 0.8714 - precision_on_1: 0.3676\n",
      "Epoch 00003: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 194s 465ms/step - loss: 0.5038 - accuracy_on_one: 0.8918 - accuracy_on_zero: 0.8714 - precision_on_1: 0.3676 - val_loss: 0.4467 - val_accuracy_on_one: 0.9056 - val_accuracy_on_zero: 0.8844 - val_precision_on_1: 0.3961\n",
      "Epoch 4/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy_on_one: 0.9002 - accuracy_on_zero: 0.8779 - precision_on_1: 0.3819\n",
      "Epoch 00004: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 194s 464ms/step - loss: 0.4738 - accuracy_on_one: 0.9002 - accuracy_on_zero: 0.8779 - precision_on_1: 0.3820 - val_loss: 0.4330 - val_accuracy_on_one: 0.9094 - val_accuracy_on_zero: 0.8871 - val_precision_on_1: 0.4028\n",
      "Epoch 5/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy_on_one: 0.9068 - accuracy_on_zero: 0.8830 - precision_on_1: 0.3939\n",
      "Epoch 00005: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.4511 - accuracy_on_one: 0.9068 - accuracy_on_zero: 0.8830 - precision_on_1: 0.3939 - val_loss: 0.4219 - val_accuracy_on_one: 0.9090 - val_accuracy_on_zero: 0.8923 - val_precision_on_1: 0.4139\n",
      "Epoch 6/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4330 - accuracy_on_one: 0.9118 - accuracy_on_zero: 0.8870 - precision_on_1: 0.4034\n",
      "Epoch 00006: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.4330 - accuracy_on_one: 0.9118 - accuracy_on_zero: 0.8870 - precision_on_1: 0.4035 - val_loss: 0.4142 - val_accuracy_on_one: 0.9111 - val_accuracy_on_zero: 0.8935 - val_precision_on_1: 0.4173\n",
      "Epoch 7/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4180 - accuracy_on_one: 0.9161 - accuracy_on_zero: 0.8904 - precision_on_1: 0.4121\n",
      "Epoch 00007: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 469ms/step - loss: 0.4180 - accuracy_on_one: 0.9161 - accuracy_on_zero: 0.8904 - precision_on_1: 0.4121 - val_loss: 0.4078 - val_accuracy_on_one: 0.9100 - val_accuracy_on_zero: 0.8970 - val_precision_on_1: 0.4253\n",
      "Epoch 8/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4046 - accuracy_on_one: 0.9199 - accuracy_on_zero: 0.8935 - precision_on_1: 0.4199\n",
      "Epoch 00008: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.4046 - accuracy_on_one: 0.9199 - accuracy_on_zero: 0.8934 - precision_on_1: 0.4199 - val_loss: 0.4027 - val_accuracy_on_one: 0.9110 - val_accuracy_on_zero: 0.8980 - val_precision_on_1: 0.4278\n",
      "Epoch 9/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy_on_one: 0.9273 - accuracy_on_zero: 0.8991 - precision_on_1: 0.4351\n",
      "Epoch 00010: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.3813 - accuracy_on_one: 0.9273 - accuracy_on_zero: 0.8991 - precision_on_1: 0.4351 - val_loss: 0.3954 - val_accuracy_on_one: 0.9083 - val_accuracy_on_zero: 0.9032 - val_precision_on_1: 0.4401\n",
      "Epoch 11/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy_on_one: 0.9300 - accuracy_on_zero: 0.9013 - precision_on_1: 0.4414\n",
      "Epoch 00011: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 467ms/step - loss: 0.3717 - accuracy_on_one: 0.9300 - accuracy_on_zero: 0.9013 - precision_on_1: 0.4414 - val_loss: 0.3937 - val_accuracy_on_one: 0.9058 - val_accuracy_on_zero: 0.9056 - val_precision_on_1: 0.4454\n",
      "Epoch 12/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy_on_one: 0.9331 - accuracy_on_zero: 0.9037 - precision_on_1: 0.4481\n",
      "Epoch 00012: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 469ms/step - loss: 0.3625 - accuracy_on_one: 0.9331 - accuracy_on_zero: 0.9036 - precision_on_1: 0.4481 - val_loss: 0.3918 - val_accuracy_on_one: 0.9059 - val_accuracy_on_zero: 0.9067 - val_precision_on_1: 0.4484\n",
      "Epoch 13/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3529 - accuracy_on_one: 0.9362 - accuracy_on_zero: 0.9060 - precision_on_1: 0.4549\n",
      "Epoch 00013: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 467ms/step - loss: 0.3529 - accuracy_on_one: 0.9362 - accuracy_on_zero: 0.9060 - precision_on_1: 0.4549 - val_loss: 0.3895 - val_accuracy_on_one: 0.9061 - val_accuracy_on_zero: 0.9079 - val_precision_on_1: 0.4517\n",
      "Epoch 14/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy_on_one: 0.9388 - accuracy_on_zero: 0.9079 - precision_on_1: 0.4609\n",
      "Epoch 00014: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 196s 469ms/step - loss: 0.3445 - accuracy_on_one: 0.9388 - accuracy_on_zero: 0.9079 - precision_on_1: 0.4609 - val_loss: 0.3883 - val_accuracy_on_one: 0.9041 - val_accuracy_on_zero: 0.9099 - val_precision_on_1: 0.4566\n",
      "Epoch 15/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy_on_one: 0.9406 - accuracy_on_zero: 0.9097 - precision_on_1: 0.4663\n",
      "Epoch 00015: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.3370 - accuracy_on_one: 0.9406 - accuracy_on_zero: 0.9097 - precision_on_1: 0.4663 - val_loss: 0.3878 - val_accuracy_on_one: 0.9029 - val_accuracy_on_zero: 0.9113 - val_precision_on_1: 0.4601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy_on_one: 0.9434 - accuracy_on_zero: 0.9119 - precision_on_1: 0.4731\n",
      "Epoch 00016: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.3283 - accuracy_on_one: 0.9434 - accuracy_on_zero: 0.9119 - precision_on_1: 0.4731 - val_loss: 0.3875 - val_accuracy_on_one: 0.8996 - val_accuracy_on_zero: 0.9141 - val_precision_on_1: 0.4671\n",
      "Epoch 17/30\n",
      "324/417 [======================>.......] - ETA: 34s - loss: 0.3222 - accuracy_on_one: 0.9456 - accuracy_on_zero: 0.9135 - precision_on_1: 0.4785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416/417 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy_on_one: 0.9650 - accuracy_on_zero: 0.9308 - precision_on_1: 0.5389\n",
      "Epoch 00027: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 194s 465ms/step - loss: 0.2559 - accuracy_on_one: 0.9650 - accuracy_on_zero: 0.9308 - precision_on_1: 0.5389 - val_loss: 0.4004 - val_accuracy_on_one: 0.8787 - val_accuracy_on_zero: 0.9283 - val_precision_on_1: 0.5065\n",
      "Epoch 28/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy_on_one: 0.9662 - accuracy_on_zero: 0.9322 - precision_on_1: 0.5443\n",
      "Epoch 00028: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 467ms/step - loss: 0.2506 - accuracy_on_one: 0.9662 - accuracy_on_zero: 0.9322 - precision_on_1: 0.5443 - val_loss: 0.4032 - val_accuracy_on_one: 0.8764 - val_accuracy_on_zero: 0.9299 - val_precision_on_1: 0.5115\n",
      "Epoch 29/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy_on_one: 0.9677 - accuracy_on_zero: 0.9338 - precision_on_1: 0.5506\n",
      "Epoch 00029: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 194s 466ms/step - loss: 0.2446 - accuracy_on_one: 0.9677 - accuracy_on_zero: 0.9338 - precision_on_1: 0.5506 - val_loss: 0.4063 - val_accuracy_on_one: 0.8734 - val_accuracy_on_zero: 0.9315 - val_precision_on_1: 0.5164\n",
      "Epoch 30/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy_on_one: 0.9695 - accuracy_on_zero: 0.9354 - precision_on_1: 0.5571\n",
      "Epoch 00030: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_input_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 195s 468ms/step - loss: 0.2389 - accuracy_on_one: 0.9695 - accuracy_on_zero: 0.9354 - precision_on_1: 0.5571 - val_loss: 0.4133 - val_accuracy_on_one: 0.8689 - val_accuracy_on_zero: 0.9337 - val_precision_on_1: 0.5234\n"
     ]
    }
   ],
   "source": [
    "history = multi_input_model.fit(dataset_multi_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=dataset_multi_val,\n",
    "                    callbacks=[cp_callback,\n",
    "                               csv_logger,\n",
    "                               #sanity_check_callback,\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load old weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights from the checkpoint path above\n",
    "# new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets (incl. image pre-processing, resizing, putting into batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save history and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "\n",
    "pickle.dump(history.history, open(training_history_path, 'wb'))\n",
    "\n",
    "# Save the model\n",
    "multi_input_model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/efs/models/Saved_models/model_Multi_input_sample_size_15000_epoch_10_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.h5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
