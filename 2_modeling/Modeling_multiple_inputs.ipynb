{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# general Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate, Input\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "# import helper functions\n",
    "from model_helper import train_test_split_custom, weighted_bce, create_dataset_multi\n",
    "from model_helper import accuracy_on_one, accuracy_on_zero\n",
    "from model_helper import precision_on_1, print_layer_trainable\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters and directories to save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Multi_with_more_dropout'\n",
    "number_of_images = 'all'\n",
    "EPOCHS = 30\n",
    "neurons_per_dense = 1024\n",
    "dense_layers = 2\n",
    "open_layers = 2\n",
    "penalty_weight = 10\n",
    "csv = '../1_cleaning/metadata_cleaned_all.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode all the categorical variables, and get the image paths\n",
    "train_test = train_test_split_custom(number_of_images, csv, multiple_input=True)\n",
    "X_train_images, X_val_images, X_train_hier, X_val_hier, y_train_bin, y_val_bin, features = train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image pipeline and create dataset to feed model\n",
    "dataset_multi_train = create_dataset_multi(X_train_images,X_train_hier, y_train_bin)\n",
    "dataset_multi_val = create_dataset_multi(X_val_images,X_val_hier, y_val_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File naming conventions\n",
    "Files should be saved in the following convention, which is outlined in the readme file\n",
    "model#_sample_size#_epoch#_dense#_trainable_layers_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the name of the model\n",
    "name_of_model = 'model_' + model_id + \\\n",
    "                '_sample_size_' + str(number_of_images) + \\\n",
    "                '_epoch_' + str(EPOCHS) + \\\n",
    "                '_dense_' + str(dense_layers) + \\\n",
    "                '_neurons_' + str(neurons_per_dense) + \\\n",
    "                '_losswbc_' + \\\n",
    "                '_num_open_layers_' + str(open_layers) + \\\n",
    "                '_penalty_weight_' + str(penalty_weight)\n",
    "\n",
    "base_path = '/home/ubuntu/efs/models/'\n",
    "    \n",
    "checkpoint_path = base_path + 'Checkpoints/' + name_of_model + '.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "csv_logger = CSVLogger(base_path + 'Training_history/' + name_of_model + \"_history_log.csv\", append=True)\n",
    "training_history_path = base_path + 'Training_history/' + name_of_model + '.pickle'\n",
    "\n",
    "saved_model_path = base_path + 'Saved_models/' + name_of_model + '.h5'\n",
    "\n",
    "model_image_path = base_path + 'Model_image/' + model_id + '.png'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the VGG19 pretrained network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the multi-input model from https://www.programcreek.com/python/example/89660/keras.layers.concatenate\n",
    "num_label = y_train_bin.shape[1]\n",
    "\n",
    "def getMultiModel():\n",
    "    \n",
    "    base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    \n",
    "    # Setup the trainability of the VGG\n",
    "    base_model.trainable = False if open_layers < 1 else True\n",
    "    \n",
    "    for layer in base_model.layers[:-(open_layers+1):]:\n",
    "        layer.trainable =  False\n",
    "    \n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(neurons_per_dense, activation='relu', name='fc_1')(x)\n",
    "    \n",
    "    input_2 = Input(shape=X_train_hier.shape[1], name=\"hier\")\n",
    "    \n",
    "    hier_layer = Dense(32, activation='relu', name = 'fc_hier')(input_2)\n",
    "    \n",
    "    merge_one = concatenate([x, hier_layer])\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    merge_one = Dense(neurons_per_dense, activation='relu', name='fc_2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(num_label, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    # Define the models\n",
    "    model = Model(inputs=[base_model.input, input_2], outputs=predictions)\n",
    "    \n",
    "    # Settings\n",
    "    LR = 1e-5\n",
    "    optimizer = Adam(lr=LR)\n",
    "    loss = weighted_bce\n",
    "    metrics = [accuracy_on_one, accuracy_on_zero, precision_on_1]\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_input_model = getMultiModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9f22c8cf60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    multi_input_model,\n",
    "    # to_file=model_image_path,\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_input_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False:\tinput_2\n",
      "False:\tblock1_conv1\n",
      "False:\tblock1_conv2\n",
      "False:\tblock1_pool\n",
      "False:\tblock2_conv1\n",
      "False:\tblock2_conv2\n",
      "False:\tblock2_pool\n",
      "False:\tblock3_conv1\n",
      "False:\tblock3_conv2\n",
      "False:\tblock3_conv3\n",
      "False:\tblock3_conv4\n",
      "False:\tblock3_pool\n",
      "False:\tblock4_conv1\n",
      "False:\tblock4_conv2\n",
      "False:\tblock4_conv3\n",
      "False:\tblock4_conv4\n",
      "False:\tblock4_pool\n",
      "False:\tblock5_conv1\n",
      "False:\tblock5_conv2\n",
      "True:\tblock5_conv3\n",
      "True:\tblock5_conv4\n",
      "True:\tblock5_pool\n",
      "True:\tflatten_1\n",
      "True:\thier\n",
      "True:\tfc_1\n",
      "True:\tfc_hier\n",
      "True:\tconcatenate_1\n",
      "True:\tdropout_2\n",
      "True:\tfc_2\n",
      "True:\tdropout_3\n",
      "True:\tdense_1\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(multi_input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_input_model.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a callback checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "    417/Unknown - 157s 375ms/step - loss: 0.8061 - accuracy_on_one: 0.8030 - accuracy_on_zero: 0.7702 - precision_on_1: 0.2379\n",
      "Epoch 00001: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 197s 472ms/step - loss: 0.8061 - accuracy_on_one: 0.8030 - accuracy_on_zero: 0.7702 - precision_on_1: 0.2379 - val_loss: 0.0000e+00 - val_accuracy_on_one: 0.0000e+00 - val_accuracy_on_zero: 0.0000e+00 - val_precision_on_1: 0.0000e+00\n",
      "Epoch 2/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy_on_one: 0.8619 - accuracy_on_zero: 0.8507 - precision_on_1: 0.3263\n",
      "Epoch 00002: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 187s 448ms/step - loss: 0.6028 - accuracy_on_one: 0.8619 - accuracy_on_zero: 0.8507 - precision_on_1: 0.3263 - val_loss: 0.4949 - val_accuracy_on_one: 0.9072 - val_accuracy_on_zero: 0.8644 - val_precision_on_1: 0.3589\n",
      "Epoch 3/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy_on_one: 0.8776 - accuracy_on_zero: 0.8644 - precision_on_1: 0.3517\n",
      "Epoch 00003: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 189s 453ms/step - loss: 0.5451 - accuracy_on_one: 0.8776 - accuracy_on_zero: 0.8644 - precision_on_1: 0.3517 - val_loss: 0.4679 - val_accuracy_on_one: 0.9140 - val_accuracy_on_zero: 0.8691 - val_precision_on_1: 0.3688\n",
      "Epoch 4/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.5117 - accuracy_on_one: 0.8871 - accuracy_on_zero: 0.8711 - precision_on_1: 0.3658\n",
      "Epoch 00004: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 450ms/step - loss: 0.5117 - accuracy_on_one: 0.8871 - accuracy_on_zero: 0.8711 - precision_on_1: 0.3658 - val_loss: 0.4500 - val_accuracy_on_one: 0.9167 - val_accuracy_on_zero: 0.8734 - val_precision_on_1: 0.3773\n",
      "Epoch 5/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4885 - accuracy_on_one: 0.8943 - accuracy_on_zero: 0.8761 - precision_on_1: 0.3769\n",
      "Epoch 00005: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 189s 453ms/step - loss: 0.4885 - accuracy_on_one: 0.8943 - accuracy_on_zero: 0.8761 - precision_on_1: 0.3769 - val_loss: 0.4377 - val_accuracy_on_one: 0.9210 - val_accuracy_on_zero: 0.8751 - val_precision_on_1: 0.3817\n",
      "Epoch 6/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4703 - accuracy_on_one: 0.8994 - accuracy_on_zero: 0.8799 - precision_on_1: 0.3856\n",
      "Epoch 00006: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4703 - accuracy_on_one: 0.8994 - accuracy_on_zero: 0.8799 - precision_on_1: 0.3856 - val_loss: 0.4283 - val_accuracy_on_one: 0.9232 - val_accuracy_on_zero: 0.8777 - val_precision_on_1: 0.3873\n",
      "Epoch 7/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy_on_one: 0.9036 - accuracy_on_zero: 0.8831 - precision_on_1: 0.3931\n",
      "Epoch 00007: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4551 - accuracy_on_one: 0.9036 - accuracy_on_zero: 0.8831 - precision_on_1: 0.3932 - val_loss: 0.4208 - val_accuracy_on_one: 0.9238 - val_accuracy_on_zero: 0.8807 - val_precision_on_1: 0.3933\n",
      "Epoch 8/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4422 - accuracy_on_one: 0.9078 - accuracy_on_zero: 0.8859 - precision_on_1: 0.4001\n",
      "Epoch 00008: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4422 - accuracy_on_one: 0.9078 - accuracy_on_zero: 0.8859 - precision_on_1: 0.4001 - val_loss: 0.4136 - val_accuracy_on_one: 0.9242 - val_accuracy_on_zero: 0.8830 - val_precision_on_1: 0.3982\n",
      "Epoch 9/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4308 - accuracy_on_one: 0.9114 - accuracy_on_zero: 0.8883 - precision_on_1: 0.4062\n",
      "Epoch 00009: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4308 - accuracy_on_one: 0.9114 - accuracy_on_zero: 0.8883 - precision_on_1: 0.4063 - val_loss: 0.4086 - val_accuracy_on_one: 0.9254 - val_accuracy_on_zero: 0.8842 - val_precision_on_1: 0.4009\n",
      "Epoch 10/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy_on_one: 0.9143 - accuracy_on_zero: 0.8905 - precision_on_1: 0.4117\n",
      "Epoch 00010: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4209 - accuracy_on_one: 0.9143 - accuracy_on_zero: 0.8905 - precision_on_1: 0.4117 - val_loss: 0.4039 - val_accuracy_on_one: 0.9259 - val_accuracy_on_zero: 0.8859 - val_precision_on_1: 0.4045\n",
      "Epoch 11/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4107 - accuracy_on_one: 0.9168 - accuracy_on_zero: 0.8929 - precision_on_1: 0.4178\n",
      "Epoch 00011: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4107 - accuracy_on_one: 0.9168 - accuracy_on_zero: 0.8929 - precision_on_1: 0.4178 - val_loss: 0.3998 - val_accuracy_on_one: 0.9260 - val_accuracy_on_zero: 0.8873 - val_precision_on_1: 0.4076\n",
      "Epoch 12/30\n",
      "416/417 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy_on_one: 0.9194 - accuracy_on_zero: 0.8949 - precision_on_1: 0.4230\n",
      "Epoch 00012: saving model to /home/ubuntu/efs/models/Checkpoints/model_Multi_with_more_dropout_sample_size_all_epoch_30_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.ckpt\n",
      "417/417 [==============================] - 188s 451ms/step - loss: 0.4022 - accuracy_on_one: 0.9195 - accuracy_on_zero: 0.8949 - precision_on_1: 0.4230 - val_loss: 0.3953 - val_accuracy_on_one: 0.9269 - val_accuracy_on_zero: 0.8894 - val_precision_on_1: 0.4124\n",
      "Epoch 13/30\n",
      "295/417 [====================>.........] - ETA: 43s - loss: 0.3949 - accuracy_on_one: 0.9218 - accuracy_on_zero: 0.8964 - precision_on_1: 0.4275"
     ]
    }
   ],
   "source": [
    "history = multi_input_model.fit(dataset_multi_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=dataset_multi_val,\n",
    "                    callbacks=[cp_callback,\n",
    "                               csv_logger,\n",
    "                               #sanity_check_callback,\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load old weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights from the checkpoint path above\n",
    "# new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets (incl. image pre-processing, resizing, putting into batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save history and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "pickle.dump(history.history, open(training_history_path, 'wb'))\n",
    "\n",
    "# Save the model\n",
    "multi_input_model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/efs/models/Saved_models/model_Multi_input_sample_size_15000_epoch_10_dense_2_neurons_1024_losswbc__num_open_layers_2_penalty_weight_10.h5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
